# ============================================================================
# BAIS Platform - Elasticsearch Configuration
# Best practices: High Availability, Performance, Security
# ============================================================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-config
  namespace: logging
  labels:
    app: elasticsearch
    component: logging
data:
  elasticsearch.yml: |
    cluster.name: bais-logs
    node.name: ${HOSTNAME}
    network.host: 0.0.0.0
    discovery.seed_hosts:
      - elasticsearch-0.elasticsearch.logging.svc.cluster.local
      - elasticsearch-1.elasticsearch.logging.svc.cluster.local
      - elasticsearch-2.elasticsearch.logging.svc.cluster.local
    cluster.initial_master_nodes:
      - elasticsearch-0
      - elasticsearch-1
      - elasticsearch-2
    path.data: /usr/share/elasticsearch/data
    path.logs: /usr/share/elasticsearch/logs
    http.port: 9200
    transport.port: 9300
    
    # Security settings
    xpack.security.enabled: false
    xpack.monitoring.collection.enabled: true
    
    # Performance settings
    bootstrap.memory_lock: true
    indices.memory.index_buffer_size: 20%
    indices.queries.cache.size: 10%
    indices.fielddata.cache.size: 20%
    
    # Index settings
    action.destructive_requires_name: true
    cluster.routing.allocation.disk.threshold_enabled: true
    cluster.routing.allocation.disk.watermark.low: 85%
    cluster.routing.allocation.disk.watermark.high: 90%
    cluster.routing.allocation.disk.watermark.flood_stage: 95%

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    component: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
        component: logging
    spec:
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - elasticsearch
              topologyKey: kubernetes.io/hostname
      initContainers:
      - name: fix-permissions
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          chown -R 1000:1000 /usr/share/elasticsearch/data
          chown -R 1000:1000 /usr/share/elasticsearch/logs
        securityContext:
          privileged: true
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: logs
          mountPath: /usr/share/elasticsearch/logs
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
        ports:
        - containerPort: 9200
          name: rest
        - containerPort: 9300
          name: inter-node
        env:
        - name: cluster.name
          value: bais-logs
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms2g -Xmx2g"
        - name: node.roles
          value: "master,data,ingest"
        - name: bootstrap.memory_lock
          value: "true"
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: logs
          mountPath: /usr/share/elasticsearch/logs
        - name: config
          mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          subPath: elasticsearch.yml
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
          limits:
            memory: "6Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        securityContext:
          capabilities:
            add:
            - IPC_LOCK
          runAsUser: 1000
          runAsGroup: 1000
          runAsNonRoot: true
      volumes:
      - name: config
        configMap:
          name: elasticsearch-config
      - name: logs
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  ports:
  - name: rest
    port: 9200
    targetPort: 9200
  - name: inter-node
    port: 9300
    targetPort: 9300
  type: ClusterIP

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: elasticsearch-cleanup
  namespace: logging
  labels:
    app: elasticsearch-cleanup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: curlimages/curl:latest
            command:
            - /bin/sh
            - -c
            - |
              set -e
              
              echo "Starting Elasticsearch index cleanup..."
              
              ELASTICSEARCH_URL="http://elasticsearch:9200"
              
              # Delete indices older than 30 days
              CUTOFF_DATE=$(date -d '30 days ago' +%Y.%m.%d)
              
              echo "Deleting indices older than $CUTOFF_DATE"
              
              # List and delete old indices
              curl -X DELETE "$ELASTICSEARCH_URL/bais-logs-*" \
                --data "{\"query\":{\"range\":{\"@timestamp\":{\"lt\":\"$CUTOFF_DATE\"}}}}"
              
              echo "Index cleanup completed"
              
            resources:
              requests:
                memory: "128Mi"
                cpu: "50m"
              limits:
                memory: "256Mi"
                cpu: "100m"
          restartPolicy: OnFailure
