# ============================================================================
# BAIS Platform - Logstash Configuration
# Best practices: Clear Pipeline, Structured Logging, Performance
# ============================================================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-config
  namespace: logging
  labels:
    app: logstash
    component: logging
data:
  logstash.yml: |
    http.host: "0.0.0.0"
    xpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]
    pipeline.workers: 2
    pipeline.batch.size: 1000
    pipeline.batch.delay: 50
    queue.type: persisted
    queue.max_bytes: 1gb
    queue.checkpoint.acks: 1024
    queue.checkpoint.writes: 1024
    queue.checkpoint.interval: 1000
    config.reload.automatic: true
    config.reload.interval: 3s
    log.level: info

  logstash.conf: |
    input {
      beats {
        port => 5044
        host => "0.0.0.0"
      }
      
      tcp {
        port => 5000
        host => "0.0.0.0"
        codec => json_lines
      }
      
      udp {
        port => 5000
        host => "0.0.0.0"
        codec => json_lines
      }
    }
    
    filter {
      # Parse Kubernetes metadata
      if [kubernetes] {
        mutate {
          add_field => { "cluster_name" => "bais-production" }
          add_field => { "environment" => "production" }
        }
        
        # Parse container logs
        if [kubernetes][labels][app] == "bais-api" {
          grok {
            match => {
              "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} %{GREEDYDATA:message}"
            }
            tag_on_failure => ["_grokparsefailure"]
          }
          
          # Parse JSON logs
          if [message] =~ /^\{.*\}$/ {
            json {
              source => "message"
              target => "log_data"
            }
          }
          
          # Parse structured logs
          if [log_data][request_id] {
            mutate {
              add_field => { "request_id" => "%{[log_data][request_id]}" }
            }
          }
          
          if [log_data][user_id] {
            mutate {
              add_field => { "user_id" => "%{[log_data][user_id]}" }
            }
          }
          
          if [log_data][endpoint] {
            mutate {
              add_field => { "endpoint" => "%{[log_data][endpoint]}" }
            }
          }
          
          if [log_data][response_time] {
            mutate {
              add_field => { "response_time" => "%{[log_data][response_time]}" }
              convert => { "response_time" => "float" }
            }
          }
          
          if [log_data][status_code] {
            mutate {
              add_field => { "status_code" => "%{[log_data][status_code]}" }
              convert => { "status_code" => "integer" }
            }
          }
          
          # Parse error logs
          if [level] == "ERROR" or [level] == "CRITICAL" {
            mutate {
              add_tag => ["error"]
            }
            
            if [message] =~ /Exception|Error|Traceback/ {
              grok {
                match => {
                  "message" => "(?m)%{DATA:error_type}: %{GREEDYDATA:error_message}"
                }
              }
            }
          }
          
          # Parse performance logs
          if [message] =~ /PERFORMANCE/ {
            mutate {
              add_tag => ["performance"]
            }
            
            grok {
              match => {
                "message" => "PERFORMANCE: %{DATA:metric_name}=%{NUMBER:metric_value:float} %{DATA:metric_unit}"
              }
            }
          }
          
          # Parse security logs
          if [message] =~ /SECURITY|AUTH|AUTHORIZATION/ {
            mutate {
              add_tag => ["security"]
            }
          }
          
          # Parse audit logs
          if [message] =~ /AUDIT/ {
            mutate {
              add_tag => ["audit"]
            }
            
            grok {
              match => {
                "message" => "AUDIT: %{DATA:action} by %{DATA:user} on %{DATA:resource} - %{DATA:result}"
              }
            }
          }
        }
        
        # Parse PostgreSQL logs
        if [kubernetes][labels][app] == "postgres-primary" {
          grok {
            match => {
              "message" => "%{TIMESTAMP:timestamp} \[%{POSINT:pid}\] %{WORD:level}: %{GREEDYDATA:message}"
            }
          }
          
          mutate {
            add_tag => ["database", "postgres"]
          }
          
          # Parse slow query logs
          if [message] =~ /slow query/ {
            mutate {
              add_tag => ["slow_query"]
            }
            
            grok {
              match => {
                "message" => "duration: %{NUMBER:duration:float} ms  statement: %{GREEDYDATA:query}"
              }
            }
          }
          
          # Parse connection logs
          if [message] =~ /connection/ {
            mutate {
              add_tag => ["connection"]
            }
          }
        }
        
        # Parse Redis logs
        if [kubernetes][labels][app] == "redis-cluster" {
          grok {
            match => {
              "message" => "%{TIMESTAMP:timestamp} %{WORD:level} %{GREEDYDATA:message}"
            }
          }
          
          mutate {
            add_tag => ["cache", "redis"]
          }
          
          # Parse memory usage logs
          if [message] =~ /memory usage/ {
            mutate {
              add_tag => ["memory"]
            }
          }
          
          # Parse eviction logs
          if [message] =~ /evicted/ {
            mutate {
              add_tag => ["eviction"]
            }
          }
        }
      }
      
      # Parse timestamp
      if [timestamp] {
        date {
          match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss" ]
          target => "@timestamp"
        }
      }
      
      # Add geo information if IP is present
      if [client_ip] {
        geoip {
          source => "client_ip"
          target => "geoip"
        }
      }
      
      # Clean up fields
      mutate {
        remove_field => [ "beat", "host", "input", "offset", "prospector" ]
      }
      
      # Add index name based on log type
      if "error" in [tags] {
        mutate {
          add_field => { "index_name" => "bais-logs-error" }
        }
      } else if "security" in [tags] {
        mutate {
          add_field => { "index_name" => "bais-logs-security" }
        }
      } else if "audit" in [tags] {
        mutate {
          add_field => { "index_name" => "bais-logs-audit" }
        }
      } else {
        mutate {
          add_field => { "index_name" => "bais-logs-app" }
        }
      }
    }
    
    output {
      elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "%{index_name}-%{+YYYY.MM.dd}"
        document_type => "_doc"
        template_name => "bais-logs"
        template_pattern => "bais-logs-*"
        template => {
          "index_patterns" => ["bais-logs-*"]
          "settings" => {
            "number_of_shards" => 1
            "number_of_replicas" => 1
            "index.refresh_interval" => "5s"
            "index.codec" => "best_compression"
          }
          "mappings" => {
            "properties" => {
              "@timestamp" => { "type" => "date" }
              "level" => { "type" => "keyword" }
              "message" => { "type" => "text", "analyzer" => "standard" }
              "logger" => { "type" => "keyword" }
              "request_id" => { "type" => "keyword" }
              "user_id" => { "type" => "keyword" }
              "endpoint" => { "type" => "keyword" }
              "response_time" => { "type" => "float" }
              "status_code" => { "type" => "integer" }
              "error_type" => { "type" => "keyword" }
              "error_message" => { "type" => "text" }
              "metric_name" => { "type" => "keyword" }
              "metric_value" => { "type" => "float" }
              "action" => { "type" => "keyword" }
              "resource" => { "type" => "keyword" }
              "result" => { "type" => "keyword" }
              "tags" => { "type" => "keyword" }
              "kubernetes" => {
                "type" => "object"
                "properties" => {
                  "namespace" => { "type" => "keyword" }
                  "pod_name" => { "type" => "keyword" }
                  "container_name" => { "type" => "keyword" }
                  "labels" => { "type" => "object" }
                }
              }
            }
          }
        }
      }
      
      # Send critical errors to alerting
      if "error" in [tags] and [level] == "CRITICAL" {
        http {
          url => "http://alertmanager:9093/api/v1/alerts"
          http_method => "post"
          format => "json"
          mapping => {
            "alerts" => [
              {
                "labels" => {
                  "alertname" => "CriticalError"
                  "service" => "%{[kubernetes][labels][app]}"
                  "namespace" => "%{[kubernetes][namespace]}"
                  "pod" => "%{[kubernetes][pod_name]}"
                }
                "annotations" => {
                  "summary" => "Critical error in BAIS application"
                  "description" => "%{message}"
                }
                "startsAt" => "%{@timestamp}"
              }
            ]
          }
        }
      }
    }

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash
  namespace: logging
  labels:
    app: logstash
    component: logging
spec:
  replicas: 2
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
        component: logging
    spec:
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
      - name: logstash
        image: docker.elastic.co/logstash/logstash:8.10.0
        ports:
        - containerPort: 5044
          name: beats
        - containerPort: 5000
          name: tcp
        env:
        - name: LS_JAVA_OPTS
          value: "-Xmx2g -Xms2g"
        volumeMounts:
        - name: config
          mountPath: /usr/share/logstash/config/logstash.yml
          subPath: logstash.yml
        - name: pipeline
          mountPath: /usr/share/logstash/pipeline/logstash.conf
          subPath: logstash.conf
        - name: data
          mountPath: /usr/share/logstash/data
        resources:
          requests:
            memory: "3Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /
            port: 9600
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /
            port: 9600
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        securityContext:
          runAsUser: 1000
          runAsGroup: 1000
          runAsNonRoot: true
      volumes:
      - name: config
        configMap:
          name: logstash-config
      - name: pipeline
        configMap:
          name: logstash-config
      - name: data
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: logstash-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 20Gi

---
apiVersion: v1
kind: Service
metadata:
  name: logstash
  namespace: logging
  labels:
    app: logstash
spec:
  selector:
    app: logstash
  ports:
  - name: beats
    port: 5044
    targetPort: 5044
  - name: tcp
    port: 5000
    targetPort: 5000
  type: ClusterIP
